# Introduction (from Description page)

How severe is an insurance claim?

When you’ve been devastated by a serious car accident, your focus is on the things that matter the most: family, friends, and other loved ones. Pushing paper with your insurance agent is the last place you want your time or mental energy spent. This is why Allstate, a personal insurer in the United States, is continually seeking fresh ideas to improve their claims service for the over 16 million households they protect.

Allstate is currently developing automated methods of predicting the cost, and hence severity, of claims. In this recruitment challenge, Kagglers are invited to show off their creativity and flex their technical chops by creating an algorithm which accurately predicts claims severity. Aspiring competitors will demonstrate insight into better ways to predict claims severity for the chance to be part of Allstate’s efforts to ensure a worry-free customer experience.

New to Kaggle? This competition is a recruiting competition, your chance to get a foot in the door with the hiring team at Allstate.

# The Data

## Introduction to the Data

Each row in this dataset represents an insurance claim. You must predict the value for the 'loss' column. Variables prefaced with 'cat' are categorical, while those prefaced with 'cont' are continuous.

File descriptions:

* dt_train.csv - the dt_training set
* test.csv - the test set. You must predict the loss value for the ids in this file.
* sample_submission.csv - a sample submission file in the correct format

## Data structure

There are no missing values.

Let's first load some packages

```{r}
library(data.table)
library(Matrix)
library(xgboost)
library(DT)
#library(plotluck) ## SHHHHHHH Kaggle docker does not have this package!!!
library(FSelector)
```

### What are their size uncompressed?

```{r, echo=TRUE, eval=FALSE}
#cat(system("ls -sh ../input/*", intern = TRUE))
cat(system("ls -sh ../input/dt_train.csv", intern = TRUE), "\n", sep = "")
cat(system("ls -sh ../input/test.csv", intern = TRUE), "\n", sep = "")
cat(system("ls -sh ../input/sample_submission.csv", intern = TRUE), "\n", sep = "")
```

```{r, echo=FALSE, eval=TRUE}
cat(system("ls -sh ../input/dt_train.csv", intern = TRUE), "\n", sep = "")
cat(system("ls -sh ../input/test.csv", intern = TRUE), "\n", sep = "")
cat(system("ls -sh ../input/sample_submission.csv", intern = TRUE), "\n", sep = "")
```

### How many lines before we even load the data?

```{r}
cat(system("cat ../input/dt_train.csv | wc -l", intern = TRUE), "\n", sep = "")
cat(system("cat ../input/test.csv | wc -l", intern = TRUE), "\n", sep = "")
cat(system("cat ../input/sample_submission.csv | wc -l", intern = TRUE), "\n", sep = "")
```

### What are the MD5 hashes?

```{r}
cat(system("md5sum ../input/dt_train.csv", intern = TRUE), "\n", sep = "")
cat(system("md5sum ../input/test.csv", intern = TRUE), "\n", sep = "")
cat(system("md5sum ../input/sample_submission.csv", intern = TRUE), "\n", sep = "")
```

### Lets setup pretty-print for the next parts

```{r}
pprint <- function(data) {
    cat(pprint_helper(data), sep = "\n")
}

pprint_helper <- function(data) {
    out <- paste(names(data), collapse = " | ")
    out <- c(out, paste(rep("---", ncol(data)), collapse = " | "))
    invisible(apply(data, 1, function(x) {
        out <<- c(out, paste(x, collapse = " | "))
    }))
    return(out)
}
```

### The data & Model

Lets look at 25 rows. Well...:

* id
* 116 categorical features
* 14 continuous features
* Loss (label to predict)

```{r, results='asis'}
dt_train <- fread("../input/dt_train.csv", header = TRUE, showProgress = FALSE, stringsAsFactors = TRUE)
test <- fread("../input/test.csv", header = TRUE, showProgress = FALSE, stringsAsFactors = TRUE)
print("  \n#### dt_train  \n  \n")
pprint(dt_train[1:25, ])
print("  \n#### Test  \n  \n")
pprint(test[1:25, ])
```

Can we see something in the id? ("loss" -> "id", ref: @Ren Zhang, @Anokas)

```{r}
plot(x = 1:nrow(dt_train), y = dt_train$loss, type = "h", main = "Loss vs row number", xlab = "Row number", ylab = "Loss")
plot(x = 1:nrow(dt_train), y = log(dt_train$loss + 1), type = "h", main = "log(Loss+1) vs row number", xlab = "Row number", ylab = "log(Loss+1)")
plot(x = dt_train$id, y = dt_train$loss, type = "h", main = "Loss vs row number", xlab = "Id", ylab = "Loss")
plot(x = dt_train$id, y = log(dt_train$loss + 1), type = "h", main = "log(Loss+1) vs row number", xlab = "Id", ylab = "log(Loss+1)")
```

Let's go ham and run a dumb xgboost! (rolling the blackbox)

```{r}
target <- dt_train$loss
dt_train <- dt_train[, loss := NULL]
data <- rbind(dt_train, test)
data <- data[, id := NULL]
gc(verbose = FALSE)
data_sparse <- sparse.model.matrix(~.-1, data = as.data.frame(data))
cat("Data size: ", data_sparse@Dim[1], " x ", data_sparse@Dim[2], "  \n", sep = "")
gc(verbose = FALSE)
ddt_train <- xgb.DMatrix(data = data_sparse[1:nrow(dt_train), ], label = target) # Create design matrix without intercept
gc(verbose = FALSE)
dtest <- xgb.DMatrix(data = data_sparse[(nrow(dt_train)+1):nrow(data), ]) # Create design matrix without intercept


gc(verbose = FALSE)
set.seed(12345678)
temp_model <- xgb.cv(data = ddt_train,
                        nthread = 8,
                        nfold = 4,
                        #nrounds = 2, # quick test
                        nrounds = 1000000,
                        max_depth = 6,
                        eta = 0.0404096, # Santander overfitting magic number X2
                        subsample = 0.70,
                        colsample_bytree = 0.70,
                        booster = "gbtree",
                        eval_metric = "mae",
                        maximize = FALSE,
                        early_stopping_rounds = 25,
                        objective = "reg:linear",
                        print_every_n = 10,
                        verbose = TRUE)

gc(verbose = FALSE)
set.seed(12345678)
temp_model <- xgb.dt_train(data = ddt_train,
                        nthread = 8,
                        #nrounds = 2, # quick test
                        nrounds = floor(temp_model$best_iteration * 1.25),
                        max_depth = 6,
                        eta = 0.0404096, # Santander overfitting magic number X2
                        subsample = 0.70,
                        colsample_bytree = 0.70,
                        booster = "gbtree",
                        eval_metric = "mae",
                        maximize = FALSE,
                        objective = "reg:linear",
                        print_every_n = 10,
                        verbose = TRUE,
                        watchlist = list(dt_train = ddt_train))
```

### Importance of features in xgboost?

@cats are smoking the ground.

```{r}
importance <- xgb.importance(feature_names = data_sparse@Dimnames[[2]], model = temp_model)

datatable(as.data.frame(importance),
          filter = "top",
          class = "cell-border stripe",
          options = list(pageLength = 20,
                         lengthMenu = c(5, 10, 15, 20, 25, 50, 100, 500))
          ) %>% formatStyle('Gain',
                             background = styleColorBar(range(importance$Gain, na.rm = TRUE, finite = TRUE), 'lightgreen'),
                             backgroundSize = '100% 90%',
                             backgroundRepeat = 'no-repeat',
                             backgroundPosition = 'center') %>%
                formatStyle('Cover',
                             background = styleColorBar(range(importance$Cover, na.rm = TRUE, finite = TRUE), 'lightblue'),
                             backgroundSize = '100% 90%',
                             backgroundRepeat = 'no-repeat',
                             backgroundPosition = 'center') %>%
                formatStyle('Frequency',
                             background = styleColorBar(range(importance$Frequency, na.rm = TRUE, finite = TRUE), 'lightgrey'),
                             backgroundSize = '100% 90%',
                             backgroundRepeat = 'no-repeat',
                             backgroundPosition = 'center') %>%
                formatPercentage(columns = c("Gain"),
                                 digits = 6) %>%
                formatPercentage(columns = c("Cover"),
                                 digits = 6) %>%
                formatPercentage(columns = c("Frequency"),
                                 digits = 6)
```

### Predictions are on the Output page

```{r}
predictedValues <- predict(temp_model, dtest)

submission <- fread("../input/sample_submission.csv", header = TRUE, showProgress = FALSE, data.table = FALSE)
submission$loss <- predictedValues
write.csv(submission, "my_submission.csv", row.names = FALSE)

# Setup for next parts
dt_train <- cbind(dt_train[, id := NULL], loss = target)
```

### Compute information gain

```{r}
mini_frame <- data.frame(matrix(nrow = 130, ncol = 2))
colnames(mini_frame) <- c("Feature", "Information_Gain")
mini_frame$Feature <- colnames(dt_train)[1:130]
gc(verbose = FALSE)
mini_data <- data.frame(matrix(nrow = nrow(dt_train), ncol = 2))
colnames(mini_data) <- c("Feature", "Loss")
mini_data$Loss <- dt_train$loss
for (i in colnames(dt_train)[1:130]) {
    mini_data$Feature <- dt_train[[i]]
    if (which(i == colnames(dt_train)[1:130]) < 117) {
        mini_data$Feature <- as.numeric(as.factor(mini_data$Feature)) # Information Gain does not care about "how it is spread"
    }
    mini_frame$Information_Gain[which(i == colnames(dt_train)[1:130])] <- information.gain(Loss ~ ., data = mini_data)$attr_importance
}
```

### Print information gain table

"Today's cute cat pic" (ref: @Megan Risdal)

Too many cats, TOO GOOD!!!!!!!!!!!!!!!!!!!!!!!!!!

```{r}
datatable(mini_frame,
          filter = "top",
          class = "cell-border stripe",
          options = list(pageLength = 20,
                         lengthMenu = c(5, 10, 15, 20, 25, 50, 100, 500),
                         order = list(list(2, "desc")))
          ) %>% formatStyle('Information_Gain',
                             background = styleColorBar(range(mini_frame$Information_Gain, na.rm = TRUE, finite = TRUE), 'lightgreen'),
                             backgroundSize = '100% 90%',
                             backgroundRepeat = 'no-repeat',
                             backgroundPosition = 'center') %>%
                formatRound(columns = c("Information_Gain"),
                            digits = 8)
```

### Now lets go on feature per feature lookup

LOL @cat116, breeding too much.

Brain power may be required for @cont2, @cont3, @cont4, @cont5, @cont8 (Continuous VS Ordinal).

Why do we fail when doing feature engineering? We only fail when we stop trying.

```{r, fig.width=10, fig.height=25}
gc(verbose = FALSE)
mini_data <- data.frame(matrix(nrow = nrow(dt_train), ncol = 2))
colnames(mini_data) <- c("Feature", "Loss")
mini_data$Loss <- dt_train$loss
par(mfrow = c(5, 2))
for (i in colnames(dt_train)[1:130]) {
    if (which(i == colnames(dt_train)[1:130]) == 71) {
        par(mfrow = c(5, 1))
    }
    #cat("  \n   \n    #### Feature ", i, "  \n  \n", sep = "")
    #if (i == "cat116") {
    #    cat("Skipped because too big T_T  \n")
    #} else {
        mini_data$Feature <- dt_train[[i]]
        if (which(i == colnames(dt_train)[1:130]) < 117) {
            mini_data$Feature <- as.factor(mini_data$Feature)
            plot(mini_data, main = paste("Loss vs ", i, sep = ""), xlab = i, ylab = "Loss")
        } else {
            plot(mini_data[!is.na(mini_data$Feature), ], main = paste("Loss vs ", i, sep = ""), xlab = i, ylab = "Loss", type = "h", xlim = range(mini_data$Feature, na.rm = TRUE, finite = TRUE))
        }
    #}
    #plotluck(loss ~ ., dt_train[, c(which(i == colnames(dt_train)), 131), with = FALSE]) # SSSHHHHH Kaggle Docker does not have this package!
}
```